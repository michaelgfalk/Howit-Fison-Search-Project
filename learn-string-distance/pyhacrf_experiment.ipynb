{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Conditional Random Fields\n",
    "\n",
    "Let's see how well Dirko Coetsee's `pyhacrf` package does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from pyhacrf import StringPairFeatureExtractor, Hacrf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as p\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "with open(\"gam_pos.p\", \"rb\") as f:\n",
    "    gam_pos = p.load(f)\n",
    "with open(\"kur_pos.p\", \"rb\") as f:\n",
    "    kur_pos = p.load(f)\n",
    "with open(\"gam_neg.p\", \"rb\") as f:\n",
    "    gam_neg = p.load(f)\n",
    "with open(\"kur_neg.p\", \"rb\") as f:\n",
    "    kur_neg = p.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyhacrf` requires that the training data come in the form of two lists:\n",
    "\n",
    "* x = a list of tuples, each of which contains two strings\n",
    "* y = a list of strings, indicating whether the tuples are a 'match' or 'non-match'\n",
    "\n",
    "To begin with, I reserve the Gamilaraay data as test data, to see if a model trained on one aboriginal language can do well on another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep five random negative matches for each anchor.\n",
    "def keep_n(df, n = 5):\n",
    "    \"\"\"\n",
    "    A little helper function that keeps n randomly selected rows from a data frame.\n",
    "    Can be used with DataFrame.groupby.apply() to keep n rows from arbitarily defined\n",
    "    groups of a data frame.\n",
    "    \n",
    "    params:\n",
    "        df: a Pandas DataFrame\n",
    "        n: the number of rows to keep\n",
    "    \n",
    "    returns:\n",
    "        selection: a shorter DataFrame with the required number of rows\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reset n if too large for this DataFrame:\n",
    "    if n > len(df):\n",
    "        n = len(df)\n",
    "    \n",
    "    # Randomly choose which rows to keep\n",
    "    rows_to_keep = np.random.choice(range(len(df)), n, replace = False)\n",
    "    \n",
    "    # Keep them\n",
    "    selection = df.iloc[rows_to_keep, :]\n",
    "    \n",
    "    return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(range(3), 1, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape training data\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# Just get juiciest training examples.\n",
    "gam_pos = gam_pos[\n",
    "    (gam_pos.pos_dist > 0.4) & # more than .4 apart in normalised Levenshtein (to avoid being too similar)\n",
    "    (gam_pos.pos_dist < 0.5) & # no more than .5 apart (to avoid false positives)\n",
    "    (gam_pos.anchor.str.len() < 10) # no more than 10 characters long (to avoid junk entries)\n",
    "]\n",
    "gam_pos = gam_pos.sample(frac = 1).reset_index(drop = True) # Shuffle training examples \n",
    "pos_iter = gam_pos[['anchor','positive']].itertuples(index = False, name = None) # Create iterator\n",
    "\n",
    "x += list(pos_iter) # Add to x list\n",
    "y += ['match' for i in range(len(gam_pos))] # Generate appropriate y labels\n",
    "\n",
    "# Get the juiciest negative examples\n",
    "gam_neg = gam_neg[\n",
    "    (gam_neg.neg_dist > 0.4) & # more than .4 apart to ensure no false negatives\n",
    "    (gam_neg.neg_dist < 0.44) # no more than .44 apart to ensure that they are no too dissimilar\n",
    "]\n",
    "gam_neg = gam_neg.groupby('anchor').apply(keep_n, n = 1) # just keep one random example per anchor word\n",
    "gam_neg = gam_neg.sample(frac = 1).reset_index(drop = True) # Shuffle training examples \n",
    "neg_iter = gam_neg[['anchor','neg_match']].itertuples(index = False, name = None)\n",
    "x += list(neg_iter) # Add to x list\n",
    "y += ['non-match' for i in range(len(gam_neg))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 881 positive examples and 947 negative ones.\n"
     ]
    }
   ],
   "source": [
    "# Applying those conditions has created a roughly equal number of positive and negative training examples:\n",
    "print(f\"There are {len(gam_pos)} positive examples and {len(gam_neg)} negative ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create charset\n",
    "# Join all the tuples into one list\n",
    "one_list = [''.join(list(x)) for x in x]\n",
    "# Put in lower case\n",
    "lowered = [x.lower() for x in one_list]\n",
    "# Join into single string and set\n",
    "charset = set(''.join(lowered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: 1463\n",
      "y_train: 1463\n",
      "x_test: 365\n",
      "y_test: 365\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "feature_extractor = StringPairFeatureExtractor(match=True, transition=True, charset = charset)\n",
    "x_extracted = feature_extractor.fit_transform(x)\n",
    "\n",
    "# Train-test split\n",
    "# Data has already been shuffled\n",
    "pos_split = int(np.ceil(len(gam_pos) * 0.8)) # Where is the 80th percentile in the positive examples?\n",
    "pos_end = len(gam_pos)\n",
    "neg_split = int(np.ceil(len(gam_neg) * 0.8)) + len(gam_pos) # Where is the 80th percentile in the negative ones?\n",
    "neg_end = len(x)\n",
    "\n",
    "# Take equal portions of the positive and negative examples\n",
    "x_train = x_extracted[0:pos_split] + x_extracted[pos_end:neg_split]\n",
    "y_train = y[0:pos_split] + y[pos_end:neg_split]\n",
    "\n",
    "x_test = x_extracted[pos_split:pos_end] + x_extracted[neg_split:neg_end]\n",
    "y_test = y[pos_split:pos_end] + y[neg_split:neg_end]\n",
    "\n",
    "# Dimensions\n",
    "print(f\"x_train: {len(x_train)}\\ny_train: {len(y_train)}\\nx_test: {len(x_test)}\\ny_test: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each training example consists of a rank-3 tensor. The rows represent the first string in the training pair, the columns the second string, and in each position is a ~1800-dimensional vector representing which characters have been switched for which in each place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 7, 1683)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  Log-likelihood |gradient|\n",
      "         0 -1.014e+03  7.996e+03\n",
      "         3  -1.01e+03  6.481e+03\n",
      "         6     -974.3   5.84e+03\n",
      "         9     -836.6  3.738e+03\n",
      "        12 -1.032e+03  3.882e+04\n",
      "        15     -698.5  2.439e+03\n",
      "        18     -639.6  1.817e+03\n",
      "        21     -630.3  2.765e+03\n",
      "        24     -620.4  1.998e+03\n",
      "        27     -661.0  3.354e+03\n",
      "        30     -589.5  2.382e+03\n",
      "        33     -580.0  1.705e+03\n",
      "        36     -560.5  1.525e+03\n",
      "        39     -557.6  1.127e+03\n",
      "        42     -556.0  1.587e+03\n",
      "        45     -551.5  1.217e+03\n",
      "        48     -549.8   1.07e+03\n",
      "        51     -548.1   1.37e+03\n",
      "        54     -538.4  1.045e+03\n",
      "        57     -536.9  1.064e+03\n",
      "        60     -536.1  1.093e+03\n",
      "        63     -533.4  3.222e+03\n",
      "        66     -530.5      970.1\n",
      "        69     -525.4      900.8\n",
      "        72     -524.3  2.731e+03\n",
      "        75     -521.4      936.7\n",
      "        78     -519.4      841.9\n",
      "        81     -517.8  1.045e+03\n",
      "        84     -516.1  1.027e+03\n",
      "        87     -513.2      727.5\n",
      "        90     -522.3  6.855e+03\n",
      "        93     -507.1      744.7\n",
      "        96     -505.0      982.6\n",
      "        99     -504.5      708.7\n",
      "       102     -502.7  1.208e+03\n",
      "       105     -501.9      667.4\n",
      "       108     -501.1      784.1\n",
      "       111     -500.3      759.0\n",
      "       114     -499.5      620.5\n",
      "       117     -497.6      676.4\n",
      "       120     -497.3      606.6\n",
      "       123     -496.7  1.058e+03\n",
      "       126     -496.4      505.9\n",
      "       129     -496.3      552.9\n",
      "       132     -594.7  2.027e+04\n",
      "       135     -536.2  3.559e+03\n",
      "       138     -495.0      668.8\n",
      "       141     -493.9      536.6\n",
      "       144     -493.5      467.3\n",
      "       147     -492.4      463.0\n",
      "       150     -491.3      440.5\n",
      "       153     -491.1      520.1\n",
      "       156     -489.3      498.2\n",
      "       159     -488.4      498.9\n",
      "       162     -488.0      485.1\n",
      "       165     -486.0      472.8\n",
      "       168     -484.9      512.7\n",
      "       171     -484.5      502.5\n",
      "       174     -483.7      392.7\n",
      "       177     -485.5  1.117e+03\n",
      "       180     -481.9      608.9\n",
      "       183     -481.7      296.1\n",
      "       186     -481.3      299.5\n",
      "       189     -481.2      271.8\n",
      "       192     -480.9      399.7\n",
      "       195     -480.3      246.2\n",
      "       198     -480.2      316.2\n",
      "       201     -480.1      293.5\n",
      "       204     -479.9      559.5\n",
      "       207     -479.6      367.5\n",
      "       210     -479.5      408.6\n",
      "       213     -526.4  7.999e+03\n",
      "       216     -479.1      251.6\n",
      "       219     -479.1      558.7\n",
      "       222     -479.0      239.8\n",
      "       225     -478.9      230.4\n",
      "       228     -482.5  2.723e+03\n",
      "       231     -478.5      372.8\n",
      "       234     -478.4      254.8\n",
      "       237     -478.3      202.6\n",
      "       240     -478.2      249.8\n",
      "       243     -478.1      255.5\n",
      "       246     -478.0      357.7\n",
      "       249     -478.0      198.4\n",
      "       252     -477.8      245.7\n",
      "       255     -477.6      183.3\n",
      "       258     -477.6      226.3\n",
      "       261     -477.5      201.1\n",
      "       264     -477.6  1.148e+03\n",
      "       267     -477.1      218.9\n",
      "       270     -476.9      181.3\n",
      "       273     -477.0      601.5\n",
      "       276     -476.6      168.8\n",
      "       279     -476.6      266.4\n",
      "       282     -476.4      263.8\n",
      "       285     -476.5      811.9\n",
      "       288     -476.3      298.8\n",
      "       291     -476.2      145.0\n",
      "       294     -476.1      178.2\n",
      "       297     -476.1      174.2\n",
      "       300     -476.0      259.2\n",
      "       303     -475.9      155.2\n",
      "       306     -475.9      159.2\n",
      "       309     -475.8      265.9\n",
      "       312     -475.7      122.6\n",
      "       315     -475.6      157.1\n",
      "       318     -475.6      385.9\n",
      "       321     -475.5      137.3\n",
      "       324     -475.4      134.0\n",
      "       327     -475.4      144.6\n",
      "       330     -475.3      139.9\n",
      "       333     -477.5  2.124e+03\n",
      "       336     -475.3      142.0\n",
      "       339     -475.2      136.8\n",
      "       342     -475.2      134.8\n",
      "       345     -475.1      159.4\n",
      "       348     -475.1      130.9\n",
      "       351     -474.9      95.91\n",
      "       354     -474.9      109.0\n",
      "       357     -475.6      791.5\n",
      "       360     -474.9      87.95\n",
      "       363     -475.5  1.474e+03\n",
      "       366     -474.8      98.12\n",
      "       369     -474.7      91.48\n",
      "       372     -474.7      220.0\n",
      "       375     -474.6      92.46\n",
      "       378     -474.6      98.98\n",
      "       381     -474.6      81.74\n",
      "       384     -474.6      122.7\n",
      "       387     -474.5      79.69\n",
      "       390     -474.5      122.7\n",
      "       393     -474.5      82.55\n",
      "       396     -474.5      91.41\n",
      "       399     -474.5      96.67\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model = Hacrf(l2_regularization=1.0)\n",
    "model.fit(x_train, y_train, verbosity = 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
