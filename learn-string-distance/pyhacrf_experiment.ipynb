{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Conditional Random Fields\n",
    "\n",
    "Let's see how well Dirko Coetsee's `pyhacrf` package does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from pyhacrf import StringPairFeatureExtractor, Hacrf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as p\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "with open(\"gam_pos.p\", \"rb\") as f:\n",
    "    gam_pos = p.load(f)\n",
    "with open(\"kur_pos.p\", \"rb\") as f:\n",
    "    kur_pos = p.load(f)\n",
    "with open(\"gam_neg.p\", \"rb\") as f:\n",
    "    gam_neg = p.load(f)\n",
    "with open(\"kur_neg.p\", \"rb\") as f:\n",
    "    kur_neg = p.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyhacrf` requires that the training data come in the form of two lists:\n",
    "\n",
    "* x = a list of tuples, each of which contains two strings\n",
    "* y = a list of strings, indicating whether the tuples are a 'match' or 'non-match'\n",
    "\n",
    "To begin with, I reserve the Gamilaraay data as test data, to see if a model trained on one aboriginal language can do well on another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep five random negative matches for each anchor.\n",
    "def keep_n(df, n = 5):\n",
    "    \"\"\"\n",
    "    A little helper function that keeps n randomly selected rows from a data frame.\n",
    "    Can be used with DataFrame.groupby.apply() to keep n rows from arbitarily defined\n",
    "    groups of a data frame.\n",
    "    \n",
    "    params:\n",
    "        df: a Pandas DataFrame\n",
    "        n: the number of rows to keep\n",
    "    \n",
    "    returns:\n",
    "        selection: a shorter DataFrame with the required number of rows\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reset n if too large for this DataFrame:\n",
    "    if n > len(df):\n",
    "        n = len(df)\n",
    "    \n",
    "    # Randomly choose which rows to keep\n",
    "    rows_to_keep = np.random.choice(range(len(df)), n, replace = False)\n",
    "    \n",
    "    # Keep them\n",
    "    selection = df.iloc[rows_to_keep, :]\n",
    "    \n",
    "    return selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(range(3), 1, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape training data\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# Just get juiciest training examples.\n",
    "gam_pos = gam_pos[\n",
    "    (gam_pos.pos_dist > 0.4) & # more than .4 apart in normalised Levenshtein (to avoid being too similar)\n",
    "    (gam_pos.pos_dist < 0.5) & # no more than .5 apart (to avoid false positives)\n",
    "    (gam_pos.anchor.str.len() < 10) # no more than 10 characters long (to avoid junk entries)\n",
    "]\n",
    "gam_pos = gam_pos.sample(frac = 1).reset_index(drop = True) # Shuffle training examples \n",
    "pos_iter = gam_pos[['anchor','positive']].itertuples(index = False, name = None) # Create iterator\n",
    "\n",
    "x += list(pos_iter) # Add to x list\n",
    "y += ['match' for i in range(len(gam_pos))] # Generate appropriate y labels\n",
    "\n",
    "# Get the juiciest negative examples\n",
    "gam_neg = gam_neg[\n",
    "    (gam_neg.neg_dist > 0.4) & # more than .4 apart to ensure no false negatives\n",
    "    (gam_neg.neg_dist < 0.44) # no more than .44 apart to ensure that they are no too dissimilar\n",
    "]\n",
    "gam_neg = gam_neg.groupby('anchor').apply(keep_n, n = 1) # just keep one random example per anchor word\n",
    "gam_neg = gam_neg.sample(frac = 1).reset_index(drop = True) # Shuffle training examples \n",
    "neg_iter = gam_neg[['anchor','neg_match']].itertuples(index = False, name = None)\n",
    "x += list(neg_iter) # Add to x list\n",
    "y += ['non-match' for i in range(len(gam_neg))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 881 positive examples and 947 negative ones.\n"
     ]
    }
   ],
   "source": [
    "# Applying those conditions has created a roughly equal number of positive and negative training examples:\n",
    "print(f\"There are {len(gam_pos)} positive examples and {len(gam_neg)} negative ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create charset\n",
    "# Join all the tuples into one list\n",
    "one_list = [''.join(list(x)) for x in x]\n",
    "# Put in lower case\n",
    "lowered = [x.lower() for x in one_list]\n",
    "# Join into single string and set\n",
    "charset = set(''.join(lowered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: 1463\n",
      "y_train: 1463\n",
      "x_test: 365\n",
      "y_test: 365\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "feature_extractor = StringPairFeatureExtractor(match=True, transition=True, charset = charset)\n",
    "x_extracted = feature_extractor.fit_transform(x)\n",
    "\n",
    "# Train-test split\n",
    "# Data has already been shuffled\n",
    "pos_split = int(np.ceil(len(gam_pos) * 0.8)) # Where is the 80th percentile in the positive examples?\n",
    "pos_end = len(gam_pos)\n",
    "neg_split = int(np.ceil(len(gam_neg) * 0.8)) + len(gam_pos) # Where is the 80th percentile in the negative ones?\n",
    "neg_end = len(x)\n",
    "\n",
    "# Take equal portions of the positive and negative examples\n",
    "x_train = x_extracted[0:pos_split] + x_extracted[pos_end:neg_split]\n",
    "y_train = y[0:pos_split] + y[pos_end:neg_split]\n",
    "\n",
    "x_test = x_extracted[pos_split:pos_end] + x_extracted[neg_split:neg_end]\n",
    "y_test = y[pos_split:pos_end] + y[neg_split:neg_end]\n",
    "\n",
    "# Dimensions\n",
    "print(f\"x_train: {len(x_train)}\\ny_train: {len(y_train)}\\nx_test: {len(x_test)}\\ny_test: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each training example consists of a rank-3 tensor. The rows represent the first string in the training pair, the columns the second string, and in each position is a ~1800-dimensional vector representing which characters have been switched for which in each place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 7, 1683)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  Log-likelihood |gradient|\n",
      "         0 -1.014e+03  7.996e+03\n",
      "         3  -1.01e+03  6.481e+03\n",
      "         6     -974.3   5.84e+03\n",
      "         9     -836.6  3.738e+03\n",
      "        12 -1.032e+03  3.882e+04\n",
      "        15     -698.5  2.439e+03\n",
      "        18     -639.6  1.817e+03\n",
      "        21     -630.3  2.765e+03\n",
      "        24     -620.4  1.998e+03\n",
      "        27     -661.0  3.354e+03\n",
      "        30     -589.5  2.382e+03\n",
      "        33     -580.0  1.705e+03\n",
      "        36     -560.5  1.525e+03\n",
      "        39     -557.6  1.127e+03\n",
      "        42     -556.0  1.587e+03\n",
      "        45     -551.5  1.217e+03\n",
      "        48     -549.8   1.07e+03\n",
      "        51     -548.1   1.37e+03\n",
      "        54     -538.4  1.045e+03\n",
      "        57     -536.9  1.064e+03\n",
      "        60     -536.1  1.093e+03\n",
      "        63     -533.4  3.222e+03\n",
      "        66     -530.5      970.1\n",
      "        69     -525.4      900.8\n",
      "        72     -524.3  2.731e+03\n",
      "        75     -521.4      936.7\n",
      "        78     -519.4      841.9\n",
      "        81     -517.8  1.045e+03\n",
      "        84     -516.1  1.027e+03\n",
      "        87     -513.2      727.5\n",
      "        90     -522.3  6.855e+03\n",
      "        93     -507.1      744.7\n",
      "        96     -505.0      982.6\n",
      "        99     -504.5      708.7\n",
      "       102     -502.7  1.208e+03\n",
      "       105     -501.9      667.4\n",
      "       108     -501.1      784.1\n",
      "       111     -500.3      759.0\n",
      "       114     -499.5      620.5\n",
      "       117     -497.6      676.4\n",
      "       120     -497.3      606.6\n",
      "       123     -496.7  1.058e+03\n",
      "       126     -496.4      505.9\n",
      "       129     -496.3      552.9\n",
      "       132     -594.7  2.027e+04\n",
      "       135     -536.2  3.559e+03\n",
      "       138     -495.0      668.8\n",
      "       141     -493.9      536.6\n",
      "       144     -493.5      467.3\n",
      "       147     -492.4      463.0\n",
      "       150     -491.3      440.5\n",
      "       153     -491.1      520.1\n",
      "       156     -489.3      498.2\n",
      "       159     -488.4      498.9\n",
      "       162     -488.0      485.1\n",
      "       165     -486.0      472.8\n",
      "       168     -484.9      512.7\n",
      "       171     -484.5      502.5\n",
      "       174     -483.7      392.7\n",
      "       177     -485.5  1.117e+03\n",
      "       180     -481.9      608.9\n",
      "       183     -481.7      296.1\n",
      "       186     -481.3      299.5\n",
      "       189     -481.2      271.8\n",
      "       192     -480.9      399.7\n",
      "       195     -480.3      246.2\n",
      "       198     -480.2      316.2\n",
      "       201     -480.1      293.5\n",
      "       204     -479.9      559.5\n",
      "       207     -479.6      367.5\n",
      "       210     -479.5      408.6\n",
      "       213     -526.4  7.999e+03\n",
      "       216     -479.1      251.6\n",
      "       219     -479.1      558.7\n",
      "       222     -479.0      239.8\n",
      "       225     -478.9      230.4\n",
      "       228     -482.5  2.723e+03\n",
      "       231     -478.5      372.8\n",
      "       234     -478.4      254.8\n",
      "       237     -478.3      202.6\n",
      "       240     -478.2      249.8\n",
      "       243     -478.1      255.5\n",
      "       246     -478.0      357.7\n",
      "       249     -478.0      198.4\n",
      "       252     -477.8      245.7\n",
      "       255     -477.6      183.3\n",
      "       258     -477.6      226.3\n",
      "       261     -477.5      201.1\n",
      "       264     -477.6  1.148e+03\n",
      "       267     -477.1      218.9\n",
      "       270     -476.9      181.3\n",
      "       273     -477.0      601.5\n",
      "       276     -476.6      168.8\n",
      "       279     -476.6      266.4\n",
      "       282     -476.4      263.8\n",
      "       285     -476.5      811.9\n",
      "       288     -476.3      298.8\n",
      "       291     -476.2      145.0\n",
      "       294     -476.1      178.2\n",
      "       297     -476.1      174.2\n",
      "       300     -476.0      259.2\n",
      "       303     -475.9      155.2\n",
      "       306     -475.9      159.2\n",
      "       309     -475.8      265.9\n",
      "       312     -475.7      122.6\n",
      "       315     -475.6      157.1\n",
      "       318     -475.6      385.9\n",
      "       321     -475.5      137.3\n",
      "       324     -475.4      134.0\n",
      "       327     -475.4      144.6\n",
      "       330     -475.3      139.9\n",
      "       333     -477.5  2.124e+03\n",
      "       336     -475.3      142.0\n",
      "       339     -475.2      136.8\n",
      "       342     -475.2      134.8\n",
      "       345     -475.1      159.4\n",
      "       348     -475.1      130.9\n",
      "       351     -474.9      95.91\n",
      "       354     -474.9      109.0\n",
      "       357     -475.6      791.5\n",
      "       360     -474.9      87.95\n",
      "       363     -475.5  1.474e+03\n",
      "       366     -474.8      98.12\n",
      "       369     -474.7      91.48\n",
      "       372     -474.7      220.0\n",
      "       375     -474.6      92.46\n",
      "       378     -474.6      98.98\n",
      "       381     -474.6      81.74\n",
      "       384     -474.6      122.7\n",
      "       387     -474.5      79.69\n",
      "       390     -474.5      122.7\n",
      "       393     -474.5      82.55\n",
      "       396     -474.5      91.41\n",
      "       399     -474.5      96.67\n",
      "       402     -474.5      109.2\n",
      "       405     -474.4      221.1\n",
      "       408     -474.4      113.1\n",
      "       411     -475.0      926.4\n",
      "       414     -474.6      653.8\n",
      "       417     -475.0      794.7\n",
      "       420     -474.4      104.7\n",
      "       423     -474.3      76.33\n",
      "       426     -474.3       70.8\n",
      "       429     -474.3      104.4\n",
      "       432     -474.3      126.0\n",
      "       435     -474.3      101.6\n",
      "       438     -474.3      72.68\n",
      "       441     -474.3      80.18\n",
      "       444     -474.2      86.11\n",
      "       447     -474.2      142.4\n",
      "       450     -474.2      72.08\n",
      "       453     -474.2       57.2\n",
      "       456     -474.2      58.76\n",
      "       459     -474.2      70.88\n",
      "       462     -474.2       94.1\n",
      "       465     -474.2       66.2\n",
      "       468     -474.2      60.75\n",
      "       471     -474.2      99.73\n",
      "       474     -474.2      70.91\n",
      "       477     -474.2      81.64\n",
      "       480     -474.2      80.66\n",
      "       483     -474.1      122.8\n",
      "       486     -474.1      94.17\n",
      "       489     -474.0      198.3\n",
      "       492     -474.0      103.0\n",
      "       495     -474.0      88.48\n",
      "       498     -474.0      59.29\n",
      "       501     -474.0      57.11\n",
      "       504     -474.0      55.82\n",
      "       507     -474.0      430.1\n",
      "       510     -474.0      76.96\n",
      "       513     -473.9      64.64\n",
      "       516     -473.9       75.7\n",
      "       519     -473.9      87.18\n",
      "       522     -473.9       84.7\n",
      "       525     -473.9      91.32\n",
      "       528     -473.9      115.1\n",
      "       531     -473.9      197.9\n",
      "       534     -473.8      64.04\n",
      "       537     -473.8       91.2\n",
      "       540     -473.8      73.91\n",
      "       543     -473.8      81.75\n",
      "       546     -475.1  1.115e+03\n",
      "       549     -473.8      75.61\n",
      "       552     -473.7      72.88\n",
      "       555     -473.7      58.39\n",
      "       558     -473.7      69.99\n",
      "       561     -473.7      63.77\n",
      "       564     -473.7      118.5\n",
      "       567     -473.7      122.7\n",
      "       570     -473.7      97.87\n",
      "       573     -473.7      58.37\n",
      "       576     -473.6      77.37\n",
      "       579     -473.6      59.74\n",
      "       582     -473.6      70.46\n",
      "       585     -473.6      58.72\n",
      "       588     -473.6      56.43\n",
      "       591     -473.6      71.77\n",
      "       594     -473.6      58.72\n",
      "       597     -473.6      202.5\n",
      "       600     -473.6      51.65\n",
      "       603     -473.5      54.92\n",
      "       606     -473.5      62.13\n",
      "       609     -473.5       45.9\n",
      "       612     -473.5      48.77\n",
      "       615     -473.5      51.19\n",
      "       618     -473.5      313.9\n",
      "       621     -473.5      52.67\n",
      "       624     -473.5       68.1\n",
      "       627     -474.8  2.055e+03\n",
      "       630     -473.5      62.19\n",
      "       633     -473.5      79.34\n",
      "       636     -473.4      86.56\n",
      "       639     -473.4      55.09\n",
      "       642     -473.4      133.3\n",
      "       645     -473.4      114.1\n",
      "       648     -473.4      67.21\n",
      "       651     -473.4      149.5\n",
      "       654     -473.4      66.41\n",
      "       657     -473.4      72.72\n",
      "       660     -473.3      85.65\n",
      "       663     -473.3      153.7\n",
      "       666     -473.3      82.62\n",
      "       669     -473.3      70.16\n",
      "       672     -473.2      73.56\n",
      "       675     -473.3      581.2\n",
      "       678     -473.2      67.67\n",
      "       681     -473.2       84.9\n",
      "       684     -473.3      608.8\n",
      "       687     -473.2      135.0\n",
      "       690     -473.2      60.85\n",
      "       693     -473.2      72.97\n",
      "       696     -473.2      70.95\n",
      "       699     -473.1      82.78\n",
      "       702     -473.1      105.4\n",
      "       705     -473.1       66.7\n",
      "       708     -473.1      70.68\n",
      "       711     -473.1      97.88\n",
      "       714     -473.1      75.04\n",
      "       717     -473.1      70.51\n",
      "       720     -473.1      67.85\n",
      "       723     -473.1       61.9\n",
      "       726     -473.1       72.9\n",
      "       729     -473.0      71.75\n",
      "       732     -473.0      91.13\n",
      "       735     -473.0      69.28\n",
      "       738     -473.0      83.99\n",
      "       741     -473.0      67.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       744     -473.0      110.7\n",
      "       747     -473.0      70.55\n",
      "       750     -473.0      64.18\n",
      "       753     -473.0      74.72\n",
      "       756     -473.0      212.5\n",
      "       759     -472.9      52.81\n",
      "       762     -472.9      46.94\n",
      "       765     -473.2  1.039e+03\n",
      "       768     -472.9      76.17\n",
      "       771     -472.9      50.43\n",
      "       774     -472.9      64.99\n",
      "       777     -472.9      55.74\n",
      "       780     -472.9      54.95\n",
      "       783     -472.9      61.52\n",
      "       786     -472.8      57.44\n",
      "       789     -472.9      269.4\n",
      "       792     -472.8      42.19\n",
      "       795     -472.8      41.12\n",
      "       798     -475.1  2.622e+03\n",
      "       801     -472.8      48.39\n",
      "       804     -472.8      138.4\n",
      "       807     -472.8      77.53\n",
      "       810     -472.8       55.6\n",
      "       813     -472.8      114.9\n",
      "       816     -472.8      44.36\n",
      "       819     -472.8      58.29\n",
      "       822     -472.8      87.95\n",
      "       825     -472.8      59.05\n",
      "       828     -472.8      42.89\n",
      "       831     -472.7      42.99\n",
      "       834     -472.7      50.84\n",
      "       837     -472.7      44.42\n",
      "       840     -472.7      39.29\n",
      "       843     -472.8      439.3\n",
      "       846     -472.7      51.98\n",
      "       849     -472.7      65.02\n",
      "       852     -472.7       81.9\n",
      "       855     -472.7       69.9\n",
      "       858     -472.7      41.55\n",
      "       861     -472.7      56.08\n",
      "       864     -472.7      33.64\n",
      "       867     -472.7      30.24\n",
      "       870     -472.7      48.35\n",
      "       873     -472.7       31.4\n",
      "       876     -472.7      33.08\n",
      "       879     -472.6      42.11\n",
      "       882     -473.1  1.031e+03\n",
      "       885     -472.6      31.42\n",
      "       888     -472.6      63.47\n",
      "       891     -472.6      30.15\n",
      "       894     -472.6      28.45\n",
      "       897     -472.6      30.36\n",
      "       900     -472.6      132.8\n",
      "       903     -472.6      28.85\n",
      "       906     -472.6      22.87\n",
      "       909     -472.6      23.97\n",
      "       912     -472.6      25.93\n",
      "       915     -472.6      131.7\n",
      "       918     -472.6      22.69\n",
      "       921     -472.6      19.61\n",
      "       924     -472.6      40.47\n",
      "       927     -472.6      29.28\n",
      "       930     -472.6       41.4\n",
      "       933     -472.6      22.66\n",
      "       936     -472.6      35.38\n",
      "       939     -472.6      21.41\n",
      "       942     -472.6      19.52\n",
      "       945     -472.6      17.92\n",
      "       948     -472.6      18.69\n",
      "       951     -472.6      19.46\n",
      "       954     -472.6      18.46\n",
      "       957     -472.6      244.9\n",
      "       960     -472.6      23.64\n",
      "       963     -472.6      17.18\n",
      "       966     -472.6       20.6\n",
      "       969     -472.6      19.45\n",
      "       972     -472.6      22.55\n",
      "       975     -472.6       54.6\n",
      "       978     -472.6      24.13\n",
      "       981     -472.6      22.32\n",
      "       984     -472.6      22.32\n",
      "       987     -472.6      14.15\n",
      "       990     -472.6      24.44\n",
      "       993     -472.6      17.55\n",
      "       996     -472.6      26.12\n",
      "       999     -472.6      17.44\n",
      "      1002     -472.6      16.46\n",
      "      1005     -472.6      72.74\n",
      "      1008     -472.6      20.95\n",
      "      1011     -472.6      23.39\n",
      "      1014     -472.6       15.5\n",
      "      1017     -472.6      18.95\n",
      "      1020     -472.6      24.49\n",
      "      1023     -472.6      18.87\n",
      "      1026     -472.6      395.8\n",
      "      1029     -472.6      18.96\n",
      "      1032     -472.6      18.28\n",
      "      1035     -472.6      27.21\n",
      "      1038     -472.6      19.42\n",
      "      1041     -472.6      18.31\n",
      "      1044     -472.6      19.88\n",
      "      1047     -472.6      22.05\n",
      "      1050     -472.6      19.43\n",
      "      1053     -472.6      13.98\n",
      "      1056     -472.6       15.3\n",
      "      1059     -472.6      23.63\n",
      "      1062     -472.6      23.53\n",
      "      1065     -472.6      18.75\n",
      "      1068     -472.6      22.37\n",
      "      1071     -472.6      26.13\n",
      "      1074     -472.6      36.22\n",
      "      1077     -472.5      27.74\n",
      "      1080     -472.5      25.54\n",
      "      1083     -472.5      37.54\n",
      "      1086     -472.5      27.38\n",
      "      1089     -472.5      25.35\n",
      "      1092     -472.5      30.22\n",
      "      1095     -472.5      35.89\n",
      "      1098     -472.5      31.84\n",
      "      1101     -472.6      481.6\n",
      "      1104     -472.5      24.66\n",
      "      1107     -472.5      29.12\n",
      "      1110     -472.5      35.87\n",
      "      1113     -472.5      27.34\n",
      "      1116     -472.5      40.93\n",
      "      1119     -472.5      42.82\n",
      "      1122     -472.5      29.81\n",
      "      1125     -472.5      40.12\n",
      "      1128     -472.5      27.47\n",
      "      1131     -472.5      30.68\n",
      "      1134     -472.6      287.9\n",
      "      1137     -472.5      24.61\n",
      "      1140     -472.5      30.64\n",
      "      1143     -472.5      24.06\n",
      "      1146     -472.5      28.65\n",
      "      1149     -472.5      30.93\n",
      "      1152     -472.5      23.94\n",
      "      1155     -472.5      34.76\n",
      "      1158     -472.5      26.57\n",
      "      1161     -472.5      27.61\n",
      "      1164     -472.5      80.18\n",
      "      1167     -472.5       27.6\n",
      "      1170     -472.5      20.43\n",
      "      1173     -472.5       22.2\n",
      "      1176     -472.5      18.85\n",
      "      1179     -472.5       20.9\n",
      "      1182     -472.5      20.64\n",
      "      1185     -472.5      24.44\n",
      "      1188     -472.5      22.15\n",
      "      1191     -472.5      17.76\n",
      "      1194     -472.5      40.14\n",
      "      1197     -472.5      17.43\n",
      "      1200     -472.5      23.81\n",
      "      1203     -472.5      15.91\n",
      "      1206     -472.5      26.32\n",
      "      1209     -472.5      199.7\n",
      "      1212     -472.5      15.89\n",
      "      1215     -472.5      17.43\n",
      "      1218     -472.5      14.68\n",
      "      1221     -472.5      42.76\n",
      "      1224     -472.5      14.45\n",
      "      1227     -472.5      24.55\n",
      "      1230     -472.5      17.12\n",
      "      1233     -472.5      12.54\n",
      "      1236     -472.5       13.1\n",
      "      1239     -472.5      19.69\n",
      "      1242     -472.5      16.88\n",
      "      1245     -472.5      16.92\n",
      "      1248     -472.5      15.16\n",
      "      1251     -472.5      22.76\n",
      "      1254     -472.5      14.72\n",
      "      1257     -472.5       13.7\n",
      "      1260     -472.5      14.53\n",
      "      1263     -472.5      26.01\n",
      "      1266     -472.4      10.74\n",
      "      1269     -472.4      13.26\n",
      "      1272     -472.4      16.92\n",
      "      1275     -472.4      12.24\n",
      "      1278     -472.4      33.46\n",
      "      1281     -472.4      11.06\n",
      "      1284     -472.4       12.2\n",
      "      1287     -472.4      10.32\n",
      "      1290     -472.4      15.08\n",
      "      1293     -472.4      9.261\n",
      "      1296     -472.4      13.97\n",
      "      1299     -472.4      8.749\n",
      "      1302     -472.4      10.78\n",
      "      1305     -472.4      8.885\n",
      "      1308     -472.4       16.5\n",
      "      1311     -472.4      6.649\n",
      "      1314     -472.4      7.952\n",
      "      1317     -472.4      7.023\n",
      "      1320     -472.4       6.59\n",
      "      1323     -472.4      8.101\n",
      "      1326     -472.4      6.623\n",
      "      1329     -472.4      9.017\n",
      "      1332     -472.4      12.95\n",
      "      1335     -472.4       5.42\n",
      "      1338     -472.4       6.02\n",
      "      1341     -472.4      5.302\n",
      "      1344     -472.4      8.315\n",
      "      1347     -472.4      5.096\n",
      "      1350     -472.4      4.609\n",
      "      1353     -472.4      5.414\n",
      "      1356     -472.4      7.742\n",
      "      1359     -472.4      5.149\n",
      "      1362     -472.4      9.058\n",
      "      1365     -472.4      7.098\n",
      "      1368     -472.4      4.749\n",
      "      1371     -472.4      5.955\n",
      "      1374     -472.4      3.868\n",
      "      1377     -472.4      4.119\n",
      "      1380     -472.4      4.119\n",
      "      1383     -472.4      3.792\n",
      "      1386     -472.4      6.116\n",
      "      1389     -472.4      3.403\n",
      "      1392     -472.4      26.74\n",
      "      1395     -472.4      109.9\n",
      "      1398     -472.4      3.572\n",
      "      1401     -472.4      3.785\n",
      "      1404     -472.4      4.324\n",
      "      1407     -472.4      3.897\n",
      "      1410     -472.4       3.85\n",
      "      1413     -472.4      4.149\n",
      "      1416     -472.4      4.932\n",
      "      1419     -472.4      3.853\n",
      "      1422     -472.4      7.999\n",
      "      1425     -472.4      6.073\n",
      "      1428     -472.4      3.829\n",
      "      1431     -472.4      3.727\n",
      "      1434     -472.4      6.972\n",
      "      1437     -472.4      8.145\n",
      "      1440     -472.4      3.803\n",
      "      1443     -472.4      2.289\n",
      "      1446     -472.4      4.663\n",
      "      1449     -472.4      3.168\n",
      "      1452     -472.4      2.323\n",
      "      1455     -472.4      4.478\n",
      "      1458     -472.4      3.347\n",
      "      1461     -472.4      2.248\n",
      "      1464     -472.4      2.485\n",
      "      1467     -472.4      1.902\n",
      "      1470     -472.4      2.299\n",
      "      1473     -472.4      3.277\n",
      "      1476     -472.4      1.772\n",
      "      1479     -472.4      1.914\n",
      "      1482     -472.4      2.216\n",
      "      1485     -472.4      1.878\n",
      "      1488     -472.4      1.832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1491     -472.4      2.546\n",
      "      1494     -472.4      2.064\n",
      "      1497     -472.4      3.388\n",
      "      1500     -472.4      2.297\n",
      "      1503     -472.4      2.108\n",
      "      1506     -472.4      4.142\n",
      "      1509     -472.4      1.935\n",
      "      1512     -472.4      1.629\n",
      "      1515     -472.4      2.096\n",
      "      1518     -472.4      1.722\n",
      "      1521     -472.4      3.932\n",
      "      1524     -472.4       1.81\n",
      "      1527     -472.4      1.521\n",
      "      1530     -472.4      1.139\n",
      "      1533     -472.4      1.336\n",
      "      1536     -472.4     0.9482\n",
      "      1539     -472.4      1.012\n",
      "      1542     -472.4      1.416\n",
      "      1545     -472.4      1.145\n",
      "      1548     -472.4      0.848\n",
      "      1551     -472.4      1.431\n",
      "      1554     -472.4      1.255\n",
      "      1557     -472.4     0.8181\n",
      "      1560     -472.4      1.287\n",
      "      1563     -472.4      1.201\n",
      "      1566     -472.4     0.7484\n",
      "      1569     -472.4     0.9212\n",
      "      1572     -472.4      1.001\n",
      "      1575     -472.4     0.7641\n",
      "      1578     -472.4      6.724\n",
      "      1581     -472.4      0.827\n",
      "      1584     -472.4     0.7409\n",
      "      1587     -472.4      2.831\n",
      "      1590     -472.4     0.9073\n",
      "      1593     -472.4     0.7507\n",
      "      1596     -472.4      0.678\n",
      "      1599     -472.4     0.5504\n",
      "      1602     -472.4     0.7484\n",
      "      1605     -472.4      1.784\n",
      "      1608     -472.4       0.53\n",
      "      1611     -472.4     0.5747\n",
      "      1614     -472.4     0.8888\n",
      "      1617     -472.4      0.958\n",
      "      1620     -472.4     0.7366\n",
      "      1623     -472.4     0.5598\n",
      "      1626     -472.4     0.5308\n",
      "      1629     -472.4     0.4743\n",
      "      1632     -472.4     0.5764\n",
      "      1635     -472.4     0.5523\n",
      "      1638     -472.4      1.102\n",
      "      1641     -472.4      1.199\n",
      "      1644     -472.4     0.6651\n",
      "      1647     -472.4     0.9551\n",
      "      1650     -472.4      5.321\n",
      "      1653     -472.4     0.6216\n",
      "      1656     -472.4     0.7438\n",
      "      1659     -472.4     0.4871\n",
      "      1662     -472.4     0.7203\n",
      "      1665     -472.4      2.068\n",
      "      1668     -472.4     0.4207\n",
      "      1671     -472.4     0.5174\n",
      "      1674     -472.4     0.4978\n",
      "      1677     -472.4     0.5335\n",
      "      1680     -472.4      3.616\n",
      "      1683     -472.4     0.6697\n",
      "      1686     -472.4     0.4524\n",
      "      1689     -472.4     0.4585\n",
      "      1692     -472.4      1.927\n",
      "      1695     -472.4     0.4632\n",
      "      1698     -472.4     0.6113\n",
      "      1701     -472.4     0.6305\n",
      "      1704     -472.4      0.402\n",
      "      1707     -472.4     0.5232\n",
      "      1710     -472.4     0.4795\n",
      "      1713     -472.4     0.4907\n",
      "      1716     -472.4      0.801\n",
      "      1719     -472.4       0.41\n",
      "      1722     -472.4     0.5643\n",
      "      1725     -472.4     0.3333\n",
      "      1728     -472.4     0.4051\n",
      "      1731     -472.4     0.3574\n",
      "      1734     -472.4     0.5167\n",
      "      1737     -472.4     0.4837\n",
      "      1740     -472.4     0.3132\n",
      "      1743     -472.4      2.411\n",
      "      1746     -472.4      0.267\n",
      "      1749     -472.4     0.2854\n",
      "      1752     -472.4     0.3007\n",
      "      1755     -472.4     0.2549\n",
      "      1758     -472.4      0.383\n",
      "      1761     -472.4     0.2936\n",
      "      1764     -472.4     0.2527\n",
      "      1767     -472.4     0.8402\n",
      "      1770     -472.4     0.2356\n",
      "      1773     -472.4      0.269\n",
      "      1776     -472.4     0.2194\n",
      "      1779     -472.4     0.2518\n",
      "      1782     -472.4      2.427\n",
      "      1785     -472.4     0.4624\n",
      "      1788     -472.4      0.434\n",
      "      1791     -472.4     0.1895\n",
      "      1794     -472.4     0.1959\n",
      "      1797     -472.4     0.2286\n",
      "      1800     -472.4     0.2235\n",
      "      1803     -472.4      0.357\n",
      "      1806     -472.4     0.6053\n",
      "      1809     -472.4     0.2353\n",
      "      1812     -472.4     0.5696\n",
      "      1815     -472.4     0.2254\n",
      "      1818     -472.4     0.1744\n",
      "      1821     -472.4     0.1593\n",
      "      1824     -472.4     0.1508\n",
      "      1827     -472.4     0.3168\n",
      "      1830     -472.4     0.1778\n",
      "      1833     -472.4     0.1475\n",
      "      1836     -472.4      0.188\n",
      "      1839     -472.4     0.2112\n",
      "      1842     -472.4     0.1468\n",
      "      1845     -472.4     0.1381\n",
      "      1848     -472.4      0.143\n",
      "      1851     -472.4      5.424\n",
      "      1854     -472.4     0.1262\n",
      "      1857     -472.4     0.1211\n",
      "      1860     -472.4     0.9465\n",
      "      1863     -472.4     0.1258\n",
      "      1866     -472.4     0.1278\n",
      "      1869     -472.4      0.239\n",
      "      1872     -472.4    0.08816\n",
      "      1875     -472.4     0.1394\n",
      "      1878     -472.4    0.09412\n",
      "      1881     -472.4     0.1148\n",
      "      1884     -472.4     0.1056\n",
      "      1887     -472.4     0.5607\n",
      "      1890     -472.4    0.09373\n",
      "      1893     -472.4     0.1341\n",
      "      1896     -472.4      1.167\n",
      "      1899     -472.4     0.1205\n",
      "      1902     -472.4    0.08491\n",
      "      1905     -472.4     0.1309\n",
      "      1908     -472.4    0.09783\n",
      "      1911     -472.4     0.1124\n",
      "      1914     -472.4    0.08899\n",
      "      1917     -472.4      0.512\n",
      "      1920     -472.4     0.1104\n",
      "      1923     -472.4     0.1251\n",
      "      1926     -472.4     0.0716\n",
      "      1929     -472.4    0.07673\n",
      "      1932     -472.4     0.1093\n",
      "      1935     -472.4     0.1112\n",
      "      1938     -472.4    0.08043\n",
      "      1941     -472.4     0.0838\n",
      "      1944     -472.4    0.09617\n",
      "      1947     -472.4    0.07138\n",
      "      1950     -472.4     0.1414\n",
      "      1953     -472.4      0.378\n",
      "      1956     -472.4     0.1248\n",
      "      1959     -472.4    0.06378\n",
      "      1962     -472.4    0.06031\n",
      "      1965     -472.4    0.05151\n",
      "      1968     -472.4    0.05938\n",
      "      1971     -472.4    0.07935\n",
      "      1974     -472.4    0.05644\n",
      "      1977     -472.4    0.06638\n",
      "      1980     -472.4    0.05031\n",
      "      1983     -472.4     0.1086\n",
      "      1986     -472.4    0.05939\n",
      "      1989     -472.4    0.05937\n",
      "      1992     -472.4    0.07418\n",
      "      1995     -472.4    0.06604\n",
      "      1998     -472.4    0.08559\n",
      "      2001     -472.4    0.06344\n",
      "      2004     -472.4     0.0821\n",
      "      2007     -472.4    0.06897\n",
      "      2010     -472.4    0.05899\n",
      "      2013     -472.4    0.05607\n",
      "      2016     -472.4    0.04958\n",
      "      2019     -472.4    0.06153\n",
      "      2022     -472.4    0.04018\n",
      "      2025     -472.4    0.04544\n",
      "      2028     -472.4    0.04339\n",
      "      2031     -472.4     0.2828\n",
      "      2034     -472.4    0.06497\n",
      "      2037     -472.4    0.03646\n",
      "      2040     -472.4    0.06943\n",
      "      2043     -472.4     0.0399\n",
      "      2046     -472.4     0.0572\n",
      "      2049     -472.4    0.06138\n",
      "      2052     -472.4    0.03432\n",
      "      2055     -472.4    0.04211\n",
      "      2058     -472.4     0.1152\n",
      "      2061     -472.4    0.03051\n",
      "      2064     -472.4    0.03241\n",
      "      2067     -472.4    0.04339\n",
      "      2070     -472.4    0.04078\n",
      "      2073     -472.4    0.03158\n",
      "      2076     -472.4    0.03315\n",
      "      2079     -472.4       0.03\n",
      "      2082     -472.4     0.0269\n",
      "      2085     -472.4    0.02464\n",
      "      2088     -472.4    0.05951\n",
      "      2091     -472.4    0.02941\n",
      "      2094     -472.4    0.03975\n",
      "      2097     -472.4    0.02174\n",
      "      2100     -472.4    0.08931\n",
      "      2103     -472.4    0.01981\n",
      "      2106     -472.4    0.01912\n",
      "      2109     -472.4    0.01857\n",
      "      2112     -472.4    0.01968\n",
      "      2115     -472.4    0.02006\n",
      "      2118     -472.4    0.02748\n",
      "      2121     -472.4    0.02087\n",
      "      2124     -472.4    0.04358\n",
      "      2127     -472.4    0.04924\n",
      "      2130     -472.4     0.0209\n",
      "      2133     -472.4    0.02252\n",
      "      2136     -472.4     0.0708\n",
      "      2139     -472.4     0.0171\n",
      "      2142     -472.4    0.02161\n",
      "      2145     -472.4    0.08061\n",
      "      2148     -472.4    0.01956\n",
      "      2151     -472.4    0.08452\n",
      "      2154     -472.4    0.02024\n",
      "      2157     -472.4    0.02244\n",
      "      2160     -472.4    0.01867\n",
      "      2163     -472.4    0.05057\n",
      "      2166     -472.4     0.0186\n",
      "      2169     -472.4    0.02229\n",
      "      2172     -472.4    0.02126\n",
      "      2175     -472.4     0.0185\n",
      "      2178     -472.4    0.01762\n",
      "      2181     -472.4    0.01443\n",
      "      2184     -472.4    0.02312\n",
      "      2187     -472.4    0.01404\n",
      "      2190     -472.4    0.01678\n",
      "      2193     -472.4    0.02212\n",
      "      2196     -472.4    0.02996\n",
      "      2199     -472.4    0.01453\n",
      "      2202     -472.4    0.02541\n",
      "      2205     -472.4    0.01313\n",
      "      2208     -472.4    0.01386\n",
      "      2211     -472.4    0.01252\n",
      "      2214     -472.4     0.0115\n",
      "      2217     -472.4    0.01295\n",
      "      2220     -472.4    0.01597\n",
      "      2223     -472.4    0.08286\n",
      "      2226     -472.4    0.01014\n",
      "      2229     -472.4    0.01164\n",
      "      2232     -472.4   0.008735\n",
      "      2235     -472.4   0.009942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2238     -472.4    0.01065\n",
      "      2241     -472.4     0.0108\n",
      "      2244     -472.4    0.04786\n",
      "      2247     -472.4   0.008558\n",
      "      2250     -472.4    0.01147\n",
      "      2253     -472.4   0.009365\n",
      "      2256     -472.4    0.00753\n",
      "      2259     -472.4   0.009133\n",
      "      2262     -472.4    0.07507\n",
      "      2265     -472.4   0.005569\n",
      "      2268     -472.4     0.1264\n",
      "      2271     -472.4   0.006693\n",
      "      2274     -472.4   0.006486\n",
      "      2277     -472.4   0.004896\n",
      "      2280     -472.4   0.006474\n",
      "      2283     -472.4   0.006788\n",
      "      2286     -472.4    0.00692\n",
      "      2289     -472.4   0.006815\n",
      "      2292     -472.4   0.004765\n",
      "      2295     -472.4   0.007478\n",
      "      2298     -472.4   0.005512\n",
      "      2301     -472.4   0.005916\n",
      "      2304     -472.4   0.003814\n",
      "      2307     -472.4    0.00396\n",
      "      2310     -472.4   0.003946\n",
      "      2313     -472.4   0.003837\n",
      "      2316     -472.4   0.004067\n",
      "      2319     -472.4   0.005478\n",
      "      2322     -472.4   0.005956\n",
      "      2325     -472.4   0.004003\n",
      "      2328     -472.4   0.004477\n",
      "      2331     -472.4    0.00352\n",
      "      2334     -472.4    0.04336\n",
      "      2337     -472.4    0.00289\n",
      "      2340     -472.4   0.004628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyhacrf.pyhacrf.Hacrf at 0x1166ec7b8>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model = Hacrf(l2_regularization=1.0)\n",
    "model.fit(x_train, y_train, verbosity = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has achieved 80% accuracy, with a precision of 0.84 and a recall of 0.75.\n"
     ]
    }
   ],
   "source": [
    "# How does the model do on the test set?\n",
    "\n",
    "# Generate a dictionary mapping class names onto numbers\n",
    "class_dict = {value:key for key,value in enumerate(model.classes)}\n",
    "\n",
    "# Get predictions on test set:\n",
    "test_pred = model.predict_proba(x_test) # Probabilities output by model\n",
    "test_pred = np.argmax(test_pred, axis = 1) # Max probability = predicted class\n",
    "\n",
    "# Apply dict to y_test\n",
    "test_classes = np.array([class_dict[x] for x in y_test], dtype='int64')\n",
    "\n",
    "# Now compare test data to predictions\n",
    "correct = test_pred == test_classes\n",
    "acc = correct.sum() / len(correct)\n",
    "\n",
    "true_positives = (test_pred == True) & (test_classes == True)\n",
    "false_positives = (test_pred == True) & (test_classes == False)\n",
    "false_negatives = (test_pred == False) & (test_classes == True)\n",
    "\n",
    "# Precision:\n",
    "precision = true_positives.sum() / (true_positives.sum() + false_positives.sum())\n",
    "recall = true_positives.sum() / (true_positives.sum() + false_negatives.sum())\n",
    "\n",
    "# The moment of truth:\n",
    "print(f\"The model has achieved {acc*100:.0f}% accuracy, with a precision of {precision:.2f} and a recall of {recall:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21493635, 0.78506365]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gk = feature_extractor.transform([(\"kookaburra\",\"gooburra\")])\n",
    "model.predict_proba(gk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the key stuff.\n",
    "to_save = {'feature_extractor':feature_extractor, 'model':model}\n",
    "\n",
    "with open('crf_trained.p', 'wb') as file:\n",
    "    p.dump(to_save, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
