{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "In this notebook, we reshape all our training data into a useful format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Levenshtein import distance\n",
    "import re\n",
    "import time\n",
    "import pickle as p\n",
    "import random\n",
    "from nltk import ngrams\n",
    "import sys\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lev_norm(str_1, str_2):\n",
    "    \"\"\"\n",
    "    Takes two strings and calculates the normalized Lev. distance between them.\n",
    "    \n",
    "    params:\n",
    "        str_1: a string\n",
    "        str_2: a string\n",
    "    \n",
    "    returns:\n",
    "        lev_norm: a number between 0 and 1\n",
    "    \"\"\"\n",
    "    dist = distance(str_1, str_2)\n",
    "    denominator = max(len(str_1), len(str_2))\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        lev_norm = dist / denominator\n",
    "        return lev_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_regex(word, n = 2, ceiling = 4):\n",
    "    \"\"\"\n",
    "    Creates a regex for blocking the data frame. It splits the word into\n",
    "    ngrams, and then will look for the first ceiling ngrams among the first\n",
    "    ceiling + 1 characters in the searched string.\n",
    "    \"\"\"\n",
    "    if len(word) < 2:\n",
    "        return \"no_regex\"\n",
    "    else:\n",
    "        grams = ngrams(list(word), n) # Outputs a generator of tuples\n",
    "        try:\n",
    "            bigrams = [''.join(x) for x in grams] # Join each tuple into a string\n",
    "        except:\n",
    "            print(f'Regex error! List of ngrams = {grams}')\n",
    "        to_keep = min(ceiling, len(bigrams)) # Keep ceilng or less of the bigrams\n",
    "        kept = '|'.join(bigrams[:to_keep]) # Join them into string\n",
    "        wrapped = '^.{,' + str(to_keep) + '}(?:' + kept + ')'\n",
    "        try:\n",
    "            regex = re.compile(wrapped, flags = re.IGNORECASE|re.UNICODE) # Paste all together into a regex\n",
    "        except:\n",
    "            return \"no_regex\"\n",
    "\n",
    "        return regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_norm(\"parlil\",\"barllil\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Howitt-Fison data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'from_the_page_out.csv'\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    how_fis = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_fis['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of categories to keep, and drop excess columns:\n",
    "to_keep = ['Social Category','Cultural/Linguistic Group','Cultural/Linguistic Group|Language Term',\n",
    "           'Kin Term','Kin Term|Language Term','Language Term','Language Term|People','Language Term|Places',\n",
    "           'Places''Social Category']\n",
    "how_fis = how_fis[how_fis.Category.isin(to_keep)]\n",
    "how_fis = how_fis[['Subject','Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of those pairs have non-identical spelling?\n",
    "how_fis[how_fis.Subject.str.lower() != how_fis.Text.str.lower()].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Austkin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austkin_path = 'austkin_out.csv'\n",
    "\n",
    "with open(austkin_path, 'r', encoding = 'iso-8859-2') as file:\n",
    "    austkin = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Chirila data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_path = 'gamilaraay_chirila_out.csv'\n",
    "kur_path = 'kurnai_chirila_out.csv'\n",
    "\n",
    "with open(gam_path, 'r', encoding = 'utf-8') as file:\n",
    "    gam = pd.read_csv(file)\n",
    "with open(kur_path, 'r', encoding = 'utf-8') as file:\n",
    "    kur = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is not already in pairs. We need to generate the pairs...\n",
    "\n",
    "### Postive pairs: group each dataframe by OriginalGloss, extract all different spellings\n",
    "\n",
    "NB: sometimes multiple spellings are in a single cell, seperated by a comma.\n",
    "\n",
    "Question: Are all words with the same gloss always the same word? Probably not always..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chirila_positive(chirila_data_frame, strictness = 0.7):\n",
    "    \"\"\"\n",
    "    Takes a dataframe of chirla data, and generates positive pairs from it.\n",
    "    \n",
    "    params:\n",
    "        chirila_data_frame: a pandas DataFrame created from a chirila csv.\n",
    "        strictness: a number between 0 and 1, the threshold for the Levenshtein test. Defaults to 0.7.\n",
    "        \n",
    "    returns:\n",
    "        chirila_positives = a pandas DataFrame of the training pairs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Catch error with chirila_data_frame\n",
    "    try:\n",
    "        assert type(chirila_data_frame) == pd.core.frame.DataFrame\n",
    "    except AssertionError:\n",
    "        print(\"Oops! That's not a DataFrame!\")\n",
    "        return AssertionError\n",
    "    \n",
    "    # Catch error with strictness.\n",
    "    if not 0 < strictness < 1:\n",
    "        strictness = 0.7\n",
    "        print(\"Strictness set outside allowable range. Reset to 0.7.\")\n",
    "    \n",
    "    # Extract all words of the same OriginalGloss:\n",
    "    positive_raw = [] # initialise empty list\n",
    "    grp = chirila_data_frame[['OriginalForm', 'OriginalGloss']].groupby('OriginalGloss') # group data\n",
    "    _ = grp.apply(lambda x: positive_raw.append(x['OriginalForm'].tolist())) # add to raw list\n",
    "    \n",
    "    # Initialise empty list for results:\n",
    "    positive_pairs = []\n",
    "\n",
    "    # Now loop through positive_raw and seperate all strings seperated by commas or semicolons\n",
    "    \n",
    "    for variants in positive_raw:\n",
    "        \n",
    "        new_list = []\n",
    "        if len(variants) == 1:  # If there is only one word in the set ...\n",
    "            continue            # ... skip it\n",
    "        for string in variants:           # Loop over each string in this list\n",
    "            if type(string) != str:\n",
    "                continue\n",
    "            string = re.sub(r'\\(.+\\)', ' ', string) # Remove parenthetical remarks\n",
    "            string = re.sub(r'\\\\|\\?|\\*|\\:|\\(|\\)', '', string) # Remove backslashes and question marks\n",
    "            string = re.sub(f'\\.', ' ', string) # Remove full stops\n",
    "            spl = re.split(\",|;| or \", string) # Split on commas, semicolons, or 'or'.\n",
    "            for substring in spl:\n",
    "                new_list.append(substring.strip()) # After splitting, add each individual word to new list\n",
    "    \n",
    "        # Now that each list has been properly tokenised, form all possible positive pairs.\n",
    "        # We are only going to keep pairs whose Levenshtein distance is less than 0.7\n",
    "        # We are also going to throw out identical words\n",
    "        while len(new_list) > 1:\n",
    "            next_word = new_list.pop() # Get next word and remove from list\n",
    "            for word in new_list:      # Now loop over all the other words.\n",
    "                if len(word) > 0:\n",
    "                    dist = lev_norm(next_word, word)\n",
    "                    new_pair = {'anchor':next_word, 'positive':word, 'pos_dist':dist} # ... add it and next_word as a training pair\n",
    "                    positive_pairs.append(new_pair)\n",
    "    \n",
    "    chirila_positives = pd.DataFrame(positive_pairs)\n",
    "    \n",
    "    # Filter out unwanted rows\n",
    "    strict = chirila_positives.pos_dist < strictness\n",
    "    not_zero = chirila_positives.pos_dist > 0\n",
    "    chirila_positives = chirila_positives[strict & not_zero]\n",
    "    chirila_positives.drop_duplicates() # drop duplicates (why are they there??)\n",
    "\n",
    "    return chirila_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_pos = generate_chirila_positive(gam, 0.7)\n",
    "kur_pos = generate_chirila_positive(kur, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chirila_negative(data, strictness = 0.5):\n",
    "    \"\"\"\n",
    "    A more efficient algorithm for finding negative training pairs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Turn off annoying warning:\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    \n",
    "    # Get bits of data we want\n",
    "    data = data[['OriginalForm','OriginalGloss']]\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Strip special characters from the gloss column\n",
    "    data['OriginalGloss'] = data['OriginalGloss'].str.replace('\\W','')\n",
    "    \n",
    "    # Initialise accumulator\n",
    "    out = pd.DataFrame(columns = ['anchor','anchor_gloss','neg_dist','OriginalForm','OriginalGloss'])\n",
    "    \n",
    "    print(\"Starting inner loop...\")\n",
    "    tick = time.perf_counter()\n",
    "    for index, row in data.iterrows():\n",
    "        \n",
    "        if index % 500 == 0:\n",
    "            tock = time.perf_counter()\n",
    "            minutes = int((tock - tick) / 60)\n",
    "            seconds = int(tock - tick) % 60\n",
    "            print(f\"Up to row {index}. {minutes} minutes and {seconds} seconds elapsed.\")\n",
    "        \n",
    "        # Get important info\n",
    "        anchor = row['OriginalForm']\n",
    "        gloss = row['OriginalGloss']\n",
    "        \n",
    "        # Skip if anchor is unique\n",
    "        if data[data.OriginalForm == anchor].shape[0] < 2:\n",
    "            continue\n",
    "\n",
    "        # Fix anchor word\n",
    "        anchor = re.sub(r'\\(.+\\)', ' ', anchor) # Remove parenthetical remarks\n",
    "        anchor = re.sub(r'\\\\|\\?|\\*|\\:|\\(|\\)', '', anchor) # Remove backslashes and question marks\n",
    "        anchor = re.sub(r'\\.', ' ', anchor) # Remove full stops\n",
    "        anchor = re.split(\",|;| or \", anchor) # Split on commas, semicolons, or 'or'.\n",
    "        anchor_list = []\n",
    "        for substring in anchor:\n",
    "            anchor_list.append(substring.strip()) # After splitting, add each individual word to new list\n",
    "\n",
    "        for a in anchor_list:\n",
    "            try:\n",
    "                # Now set up search:\n",
    "                regex = n_gram_regex(a)\n",
    "                if type(regex) != re.Pattern: # Skip if no regex found\n",
    "                    continue\n",
    "\n",
    "                field = data[\n",
    "                    ~data['OriginalGloss'].str.contains(gloss) & # keep words with different gloss\n",
    "                    data['OriginalForm'].str.contains(regex) # keep words that match the regex\n",
    "                ]\n",
    "                \n",
    "                # Skip if there are no possible matches\n",
    "                if field.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                # Compute normalised Levenshtein distance with all of them\n",
    "                field['neg_dist'] = field.apply(lambda x: lev_norm(a, x['OriginalForm']), axis = 1)\n",
    "\n",
    "                # Filter according to Goldilocks\n",
    "                field = field[field.neg_dist > 0.1] # Too close and they might actually be the same word\n",
    "                field = field[field.neg_dist < strictness] # Too far and they will be too easy to distinguish\n",
    "\n",
    "                # Keep unique forms and glosses\n",
    "                field = field.groupby('OriginalForm', as_index = False).first()\n",
    "                field = field.groupby('OriginalGloss', as_index = False).first()\n",
    "                field['anchor'] = a\n",
    "                field['anchor_gloss'] = gloss\n",
    "\n",
    "                # Merge into output\n",
    "                out = out.append(field, sort = False)\n",
    "            except:\n",
    "                return(a, index, regex, out, sys.exc_info())\n",
    "\n",
    "    tock = time.perf_counter()\n",
    "    minutes = int((tock - tick) / 60)\n",
    "    seconds = int(tock - tick) % 60\n",
    "    print(f\"Inner loop complete. It took {minutes} minutes and {seconds} seconds.\")\n",
    "    out = out.rename(columns = {'OriginalForm':'neg_match'})\n",
    "    out = out.rename(columns = {'OriginalGloss':'neg_gloss'})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_neg = generate_chirila_negative(gam, strictness = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kur_neg = generate_chirila_negative(kur, strictness = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{gam_pos.shape[0]} positive matches, and {gam_neg.shape[0]} negative matches were found for Gamilaraay.\")\n",
    "print(f\"{kur_pos.shape[0]} positive matches, and {kur_neg.shape[0]} negative matches were found for Gunnaikurnai.\\n\")\n",
    "# How many matches per anchor on average?\n",
    "gam_avg = gam_neg.groupby(by = \"anchor\").size().mean()\n",
    "kur_avg = kur_neg.groupby(by = \"anchor\").size().mean()\n",
    "print(f\"{gam_avg:.2f} negative matches were found for each Gamilaraay anchor on average.\")\n",
    "print(f\"{kur_avg:.2f} negative matches were found for each Gunnaikurnai anchor on average.\\n\")\n",
    "\n",
    "print(f\"The mean Levenshtein distances (normalised) for Gamillaraay were:\\nPositive examples: {gam_pos.pos_dist.mean():.3f}\\nNegative examples: {gam_neg.neg_dist.mean():.3f}\\n\")\n",
    "print(f\"And for Gunnaikurnai:\\nPositive examples: {kur_pos.pos_dist.mean():.3f}\\nNegative examples: {kur_neg.neg_dist.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gam_pos.p\", \"wb\") as f:\n",
    "    p.dump(gam_pos, f)\n",
    "with open(\"gam_neg.p\", \"wb\") as f:\n",
    "    p.dump(gam_neg, f)\n",
    "with open(\"kur_pos.p\", \"wb\") as f:\n",
    "    p.dump(kur_pos, f)\n",
    "with open(\"kur_neg.p\", \"wb\") as f:\n",
    "    p.dump(kur_neg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moment of truth. Having generated data frames of positive and negative pairs, the time has come to join them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_triples(data_frame, n = 30):\n",
    "    \"\"\"\n",
    "    This function helps keep the size of the training set manageable, by only keeping the n-closest negative pairs.\n",
    "    \"\"\"\n",
    "    df = data_frame.groupby(['anchor','positive'], as_index = False).apply(lambda x: x.nsmallest(30, 'neg_dist'))\n",
    "    df = df.reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam_triples = pd.merge(gam_pos, gam_neg, how = 'outer')\n",
    "gam_triples = gam_triples.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gam_triples.p\", \"wb\") as gam_p:\n",
    "    p.dump(gam_triples, gam_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kur_triples = pd.merge(kur_pos, kur_neg, how = 'outer')\n",
    "kur_triples = kur_triples.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kur_triples.p\", \"wb\") as kur_p:\n",
    "    p.dump(kur_triples, kur_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How did we do?\n",
    "print(f\"{len(gam_triples)} training triples were generated for Gamilaraay.\")\n",
    "print(f\"{len(kur_triples)} were generated for Gunnaikurnai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Vectorise the strings\n",
    "\n",
    "The final step is to create a vectoriser and vectorise all the strings in the corpus, so that it can be used for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data if in a new session\n",
    "with open(\"gam_triples.p\", \"rb\") as gam_p:\n",
    "    gam_triples = p.load(gam_p)\n",
    "with open(\"kur_triples.p\", \"rb\") as kur_p:\n",
    "    kur_triples = p.load(kur_p)\n",
    "\n",
    "all_triples = gam_triples.append(kur_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_reduced = reduce_triples(all_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize tokenizer\n",
    "one_hot_encoder = Tokenizer(filters = None, char_level = True)\n",
    "\n",
    "# Fit to corpus\n",
    "all_words = set(gam.OriginalForm.astype('str')).union(set(kur.OriginalForm.astype('str')))\n",
    "one_hot_encoder.fit_on_texts(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to sequences and pad\n",
    "anchor_seq = one_hot_encoder.texts_to_sequences(triples_reduced.anchor)\n",
    "anchor_seq = pad_sequences(anchor_seq, maxlen = 10, padding = 'post')\n",
    "\n",
    "positive_seq = one_hot_encoder.texts_to_sequences(triples_reduced.positive)\n",
    "positive_seq = pad_sequences(positive_seq, maxlen = 10, padding = 'post')\n",
    "\n",
    "negative_seq = one_hot_encoder.texts_to_sequences(triples_reduced.neg_match)\n",
    "negative_seq = pad_sequences(negative_seq, maxlen = 10, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join sequences into 3d array\n",
    "data_tensor = np.stack([anchor_seq, positive_seq, negative_seq], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get character dictionary from encoder\n",
    "char_dict = one_hot_encoder.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into dictionary and export\n",
    "out = {\"data\":data_tensor, \"char_dict\":char_dict}\n",
    "\n",
    "with open(\"data.p\", \"wb\") as f:\n",
    "    p.dump(out, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
